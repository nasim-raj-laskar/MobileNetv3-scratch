# MobileNetV3-Large: Complete Implementation Notes

> **üìì For detailed understanding and complete quick reference, see:** [`notebook/MV3_scratch_complete_notes.ipynb`](notebook/MV3_scratch_complete_notes.ipynb)
> ‚ö†Ô∏è This implementation is intended for educational and architectural understanding.
> For production use or pretrained weights, refer to official TensorFlow implementations.


A from-scratch, block-level implementation of MobileNetV3-Large in TensorFlow, combining detailed theory with a faithful architectural reconstruction.
---

## üìö Table of Contents

1. [Introduction & Motivation](#introduction--motivation)
2. [Evolution: MobileNet Family](#evolution-mobilenet-family)
3. [Core Concepts & Theory](#core-concepts--theory)
4. [Architecture Deep Dive](#architecture-deep-dive)
5. [Implementation Details](#implementation-details)
6. [Usage & Training](#usage--training)
7. [References](#references)

---

## üéØ Introduction & Motivation

### Why MobileNets?

Traditional CNNs (VGG, ResNet) achieve high accuracy but are computationally expensive:
- **VGG16:** ~138M parameters, ~15.5 GFLOPs
- **ResNet50:** ~25M parameters, ~4 GFLOPs

Mobile and edge devices have constraints:
- Limited computational power (CPU/GPU)
- Battery life considerations
- Memory constraints
- Real-time inference requirements

**MobileNets solve this** by achieving comparable accuracy with significantly fewer parameters and operations.

### MobileNetV3 Goals

1. **Efficiency:** Reduce FLOPs and latency
2. **Accuracy:** Maintain competitive performance
3. **Flexibility:** Adaptable to different hardware platforms
4. **Automation:** Use Neural Architecture Search (NAS) for optimization

---

## üîÑ Evolution: MobileNet Family

### MobileNetV1 (2017)
- Introduced **Depthwise Separable Convolutions**
- Reduced parameters by 8-9x compared to standard convolutions
- Width multiplier (Œ±) and resolution multiplier (œÅ) for scaling

### MobileNetV2 (2018)
- Added **Inverted Residual Blocks** (bottleneck structure)
- **Linear Bottlenecks** (no activation on projection layer)
- Residual connections for gradient flow
- Expansion factor for expressiveness

### MobileNetV3 (2019) - Current Implementation
- **Neural Architecture Search (NAS)** for block-level optimization
- **Squeeze-and-Excitation (SE)** attention mechanisms
- **Hard-Swish** activation (efficient alternative to Swish)
- Redesigned expensive layers (initial and final)
- Two variants: Large (accuracy) and Small (efficiency)

---

## üß† Core Concepts & Theory

### 1. Depthwise Separable Convolutions

**Standard Convolution:**
- Input: H √ó W √ó C_in
- Kernel: K √ó K √ó C_in √ó C_out
- Operations: H √ó W √ó K √ó K √ó C_in √ó C_out

**Depthwise Separable = Depthwise + Pointwise:**

**Depthwise (spatial filtering):**
- One filter per input channel
- Operations: H √ó W √ó K √ó K √ó C_in

**Pointwise (channel mixing):**
- 1√ó1 convolution
- Operations: H √ó W √ó C_in √ó C_out

**Computational Savings:**
```
Reduction = (K¬≤ √ó C_in √ó C_out) / (K¬≤ √ó C_in + C_in √ó C_out)
         ‚âà 1/C_out + 1/K¬≤
```
For K=3, C_out=256: ~8-9x reduction!

### 2. Inverted Residual Block (Bottleneck)

**Traditional Residual (ResNet):**
- Wide ‚Üí Narrow ‚Üí Wide (compress then expand)
- Residual: learns difference from identity

**Inverted Residual (MobileNetV2/V3):**
- Narrow ‚Üí Wide ‚Üí Narrow (expand then compress)

**Structure:**
```
Input (low channels)
    ‚Üì
[1√ó1 Conv] Expansion (increase channels 4-6x)
    ‚Üì
[3√ó3/5√ó5 Depthwise] Spatial filtering
    ‚Üì
[SE Block] Optional attention
    ‚Üì
[1√ó1 Conv] Projection (reduce channels, LINEAR)
    ‚Üì
[Add] Residual connection (if stride=1 and same channels)
    ‚Üì
Output (low channels)
```

**Why Inverted?**
- Depthwise convolutions are efficient but have limited expressiveness
- Expand to higher dimensions for richer representations
- Compress back to save memory
- Linear projection preserves information (no ReLU destroying negative values)

### 3. Squeeze-and-Excitation (SE) Blocks

**Channel Attention Mechanism:**

Learns to emphasize important channels and suppress less useful ones.

**Process:**
1. **Squeeze:** Global Average Pooling ‚Üí (H,W,C) ‚Üí (1,1,C)
   - Aggregates spatial information per channel
   
2. **Excitation:** Two FC layers
   - FC1: C ‚Üí C/r (reduction, typically r=4)
   - ReLU activation
   - FC2: C/r ‚Üí C
   - Hard-Sigmoid activation (outputs 0-1 weights)
   
3. **Scale:** Channel-wise multiplication
   - Original features √ó learned weights

**Mathematical Formulation:**
```
z = GlobalAvgPool(x)                    # Squeeze
s = œÉ(W‚ÇÇ ¬∑ ReLU(W‚ÇÅ ¬∑ z))               # Excitation
xÃÉ = x ‚äô s                              # Scale
```

**Benefits:**
- Minimal parameters (~5% increase)
- Significant accuracy improvement
- Adaptive feature recalibration

### 4. Activation Functions

**ReLU (Rectified Linear Unit):**
```
ReLU(x) = max(0, x)
```
- Simple, fast
- Used in early stages

**Swish:**
```
Swish(x) = x ¬∑ œÉ(x)
```
- Smooth, non-monotonic
- Better accuracy but computationally expensive

**Hard-Swish (MobileNetV3):**
```
h-swish(x) = x ¬∑ ReLU6(x + 3) / 6
```
- Piecewise linear approximation of Swish
- Hardware-friendly (no exponentials)
- Used in deeper layers where accuracy matters more

**ReLU6:**
```
ReLU6(x) = min(max(0, x), 6)
```
- Bounded output for quantization robustness

---

## üèóÔ∏è Architecture Deep Dive

### Overall Structure

```
Input (224√ó224√ó3)
    ‚Üì
[Initial Conv] 3√ó3, stride=2 ‚Üí 112√ó112√ó16
    ‚Üì
[Stage 1] 3 Bottleneck blocks ‚Üí 56√ó56√ó24
    ‚Üì
[Stage 2] 3 Bottleneck blocks ‚Üí 28√ó28√ó40
    ‚Üì
[Stage 3] 6 Bottleneck blocks ‚Üí 14√ó14√ó112
    ‚Üì
[Stage 4] 3 Bottleneck blocks ‚Üí 7√ó7√ó160
    ‚Üì
[Final Conv] 1√ó1 ‚Üí 7√ó7√ó960
    ‚Üì
[Global Avg Pool] ‚Üí 1√ó1√ó960
    ‚Üì
[Conv 1√ó1] ‚Üí 1√ó1√ó1280
    ‚Üì
[Dropout 0.8]
    ‚Üì
[Classifier] ‚Üí num_classes
```

### Stage-by-Stage Breakdown

#### Initial Layer
- **Conv 3√ó3, stride=2:** 224√ó224√ó3 ‚Üí 112√ó112√ó16
- Hard-Swish activation
- Reduces spatial dimensions early

#### Stage 1: Shallow Feature Learning (56√ó56)
| Block | Input Ch | Exp Ch | Output Ch | Kernel | Stride | SE | Act |
|-------|----------|--------|-----------|--------|--------|----|----|
| 1 | 16 | 16 | 16 | 3√ó3 | 1 | ‚úó | ReLU |
| 2 | 16 | 64 | 24 | 3√ó3 | 2 | ‚úó | ReLU |
| 3 | 24 | 72 | 24 | 3√ó3 | 1 | ‚úó | ReLU |

**Characteristics:**
- Small kernels (3√ó3) for basic features
- ReLU activation (efficiency priority)
- No SE blocks (early features don't need attention)
- Expansion ratios: 1x, 4x, 3x

#### Stage 2: Medium Features (28√ó28)
| Block | Input Ch | Exp Ch | Output Ch | Kernel | Stride | SE | Act |
|-------|----------|--------|-----------|--------|--------|----|----|
| 4 | 24 | 72 | 40 | 5√ó5 | 2 | ‚úì | ReLU |
| 5 | 40 | 120 | 40 | 5√ó5 | 1 | ‚úì | ReLU |
| 6 | 40 | 120 | 40 | 5√ó5 | 1 | ‚úì | ReLU |

**Characteristics:**
- Larger kernels (5√ó5) for broader receptive field
- SE blocks introduced (channel attention)
- Still using ReLU
- Expansion ratio: 3x

#### Stage 3: Deep Representations (14√ó14)
| Block | Input Ch | Exp Ch | Output Ch | Kernel | Stride | SE | Act |
|-------|----------|--------|-----------|--------|--------|----|----|
| 7 | 40 | 240 | 80 | 3√ó3 | 2 | ‚úó | H-Swish |
| 8 | 80 | 200 | 80 | 3√ó3 | 1 | ‚úó | H-Swish |
| 9 | 80 | 184 | 80 | 3√ó3 | 1 | ‚úó | H-Swish |
| 10 | 80 | 184 | 80 | 3√ó3 | 1 | ‚úó | H-Swish |
| 11 | 80 | 480 | 112 | 3√ó3 | 1 | ‚úì | H-Swish |
| 12 | 112 | 672 | 112 | 3√ó3 | 1 | ‚úì | H-Swish |

**Characteristics:**
- Switch to Hard-Swish (accuracy matters more)
- Mixed SE usage (NAS-optimized)
- Higher expansion ratios (6x)
- Most blocks in this stage

#### Stage 4: High-Level Features (7√ó7)
| Block | Input Ch | Exp Ch | Output Ch | Kernel | Stride | SE | Act |
|-------|----------|--------|-----------|--------|--------|----|----|
| 13 | 112 | 672 | 160 | 5√ó5 | 2 | ‚úì | H-Swish |
| 14 | 160 | 960 | 160 | 5√ó5 | 1 | ‚úì | H-Swish |
| 15 | 160 | 960 | 160 | 5√ó5 | 1 | ‚úì | H-Swish |

**Characteristics:**
- Large kernels (5√ó5) for global context
- All blocks have SE (critical features)
- Hard-Swish activation
- Highest expansion ratio (6x)

#### Final Layers
```
Conv 1√ó1: 160 ‚Üí 960 (feature fusion)
Global Avg Pool: 7√ó7√ó960 ‚Üí 1√ó1√ó960
Conv 1√ó1: 960 ‚Üí 1280 (final representation)
Dropout: 0.8 (regularization)
Conv 1√ó1: 1280 ‚Üí num_classes (classifier)
```

### Design Principles

1. **Progressive Complexity:**
   - Early: Simple, efficient (ReLU, no SE)
   - Late: Complex, accurate (H-Swish, SE)

2. **Efficient Downsampling:**
   - Stride=2 in specific blocks
   - Reduces spatial dimensions: 224‚Üí112‚Üí56‚Üí28‚Üí14‚Üí7

3. **Channel Evolution:**
   - Gradual increase: 16‚Üí24‚Üí40‚Üí80‚Üí112‚Üí160
   - Expansion in bottlenecks for expressiveness

4. **NAS-Optimized:**
   - Kernel sizes, SE placement, expansion ratios
   - Found through automated search

---

## üíª Implementation Details

### Project Structure

```
mobilenetv3-scratch/
‚îú‚îÄ‚îÄ mobilenetv3L/
‚îÇ   ‚îú‚îÄ‚îÄ model.py          # Main architecture (15 bottleneck blocks)
‚îÇ   ‚îú‚îÄ‚îÄ bottleneck.py     # Inverted residual block implementation
‚îÇ   ‚îú‚îÄ‚îÄ conv.py           # Conv + BN + Activation block
‚îÇ   ‚îú‚îÄ‚îÄ se.py             # Squeeze-and-Excitation attention
‚îÇ   ‚îú‚îÄ‚îÄ activations.py    # Hard-Swish function
‚îÇ   ‚îî‚îÄ‚îÄ utils.py          # Helper functions
‚îú‚îÄ‚îÄ assets/               # Architecture diagrams
‚îú‚îÄ‚îÄ notebook/
‚îÇ   ‚îî‚îÄ‚îÄ MV3_scratch_complete_notes.ipynb  # üìì Detailed walkthrough
‚îú‚îÄ‚îÄ model_summary.py      # Display architecture
‚îú‚îÄ‚îÄ train.py              # Training template
‚îî‚îÄ‚îÄ README.md
```

### Key Implementation Choices

**1. Bottleneck Block (BNeck):**
```python
def BNeck(x, in_ch, exp_ch, out_ch, kernel, stride, use_se, activation):
    # Expansion
    x_exp = ConvBlock(x, exp_ch, 1, act=activation)
    
    # Depthwise
    x_dw = DepthwiseConv2D(kernel, strides=stride)(x_exp)
    x_dw = BatchNormalization()(x_dw)
    x_dw = Activation(activation)(x_dw)
    
    # SE Block (optional)
    if use_se:
        x_dw = SEBlock(x_dw, exp_ch)
    
    # Projection (LINEAR - no activation)
    x_proj = Conv2D(out_ch, 1)(x_dw)
    x_proj = BatchNormalization()(x_proj)
    
    # Residual connection
    if stride == 1 and in_ch == out_ch:
        x_proj = Add()([x_proj, x])
    
    return x_proj
```

**2. SE Block:**
```python
def SEBlock(x, filters, reduction=4):
    se = GlobalAveragePooling2D()(x)
    se = Dense(filters // reduction, activation='relu')(se)
    se = Dense(filters, activation='hard_sigmoid')(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([x, se])
```

**3. Hard-Swish:**
```python
def hard_swish(x):
    return x * tf.nn.relu6(x + 3) / 6
```

### Model Specifications

- **Parameters:** ~5.4M
- **FLOPs:** ~219M (at 224√ó224)
- **Input:** 224√ó224√ó3 (RGB images)
- **Output:** Logits (num_classes)
- **Dropout:** 0.8 before classifier
- **Batch Normalization:** After every convolution

---

## üöÄ Usage & Training

### Installation

```bash
pip install tensorflow numpy
```

### Quick Start

```python
from mobilenetv3L.model import MobileNetV3_Large

# Create model
model = MobileNetV3_Large(
    input_shape=(224, 224, 3),
    num_classes=1000
)

# View architecture
model.summary()
```

### Training Example

```python
import tensorflow as tf
from mobilenetv3L.model import MobileNetV3_Large

# Initialize
model = MobileNetV3_Large(input_shape=(224, 224, 3), num_classes=10)

# Compile (IMPORTANT: from_logits=True)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Train
history = model.fit(
    train_dataset,
    epochs=50,
    validation_data=val_dataset,
    callbacks=[
        tf.keras.callbacks.ReduceLROnPlateau(patience=3),
        tf.keras.callbacks.EarlyStopping(patience=5)
    ]
)
```

### Inference

```python
import numpy as np
from tensorflow.keras.preprocessing import image

# Load image
img = image.load_img('image.jpg', target_size=(224, 224))
img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

# Predict (logits)
logits = model.predict(img_array)

# Convert to probabilities
probabilities = tf.nn.softmax(logits)
predicted_class = np.argmax(probabilities, axis=1)
```

### Transfer Learning

```python
# Load base model
base_model = MobileNetV3_Large(input_shape=(224, 224, 3), num_classes=1000)

# Remove classifier
x = base_model.layers[-3].output  # Before dropout

# Add custom classifier
x = layers.Dropout(0.5)(x)
x = layers.Dense(num_custom_classes)(x)
outputs = layers.Activation('softmax')(x)

# Create new model
custom_model = models.Model(inputs=base_model.input, outputs=outputs)

# Freeze base layers
for layer in base_model.layers[:-10]:
    layer.trainable = False
```

---

## üîç Important Notes

### 1. Logits vs Probabilities
- Model outputs **logits** (raw scores)
- Use `from_logits=True` in loss function
- Apply `softmax` for probabilities during inference

### 2. Batch Normalization
- All convolutions followed by BN
- Helps with training stability
- Reduces internal covariate shift

### 3. Linear Bottlenecks
- No activation after projection layer
- Preserves information in low-dimensional space
- Critical for inverted residual design

### 4. Residual Connections
- Only when `stride=1` and `in_channels == out_channels`
- Enables gradient flow
- Improves training convergence

### 5. Dropout Placement
- Only before final classifier (0.8 rate)
- Prevents overfitting
- Not used in bottleneck blocks

---

## üìñ References

### Papers
1. **MobileNetV3:** [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244) - Howard et al., 2019
2. **MobileNetV2:** [Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) - Sandler et al., 2018
3. **MobileNetV1:** [Efficient Convolutional Neural Networks](https://arxiv.org/abs/1704.04861) - Howard et al., 2017
4. **SE Networks:** [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507) - Hu et al., 2018
5. **Swish Activation:** [Searching for Activation Functions](https://arxiv.org/abs/1710.05941) - Ramachandran et al., 2017

### Additional Resources
- [TensorFlow MobileNet Guide](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV3Large)
- [Depthwise Separable Convolutions Explained](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)
- [Neural Architecture Search Overview](https://arxiv.org/abs/1808.05377)

---

## üìì Complete Tutorial

**For step-by-step implementation with code explanations, visualizations, and detailed notes:**

üëâ **See:** [`notebook/MV3_scratch_complete_notes.ipynb`](notebook/MV3_scratch_complete_notes.ipynb)

The notebook includes:
- Line-by-line code walkthrough
- Architecture visualizations
- Mathematical derivations
- Computational complexity analysis
- Training tips and best practices
- Comparison with other architectures

---

## üéì Learning Path

1. **Start here:** Read this README for theoretical foundation
2. **Deep dive:** Work through the Jupyter notebook
3. **Experiment:** Run `model_summary.py` to see architecture
4. **Practice:** Modify `train.py` for your dataset
5. **Explore:** Examine individual module implementations

---

**Built with TensorFlow 2.x | Educational Implementation | From Scratch**
